{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "target=[]\n",
    "with open('train.csv') as dat:\n",
    "    dat.readline()\n",
    "    reader = csv.reader(dat, delimiter = ',')\n",
    "    for row in reader:\n",
    "        temp=cv2.imread(os.path.join(os.getcwd(), 'train', row[0]))\n",
    "        temp1=cv2.resize(temp, (100,100))\n",
    "        train_data.append(temp1)\n",
    "        target.append(row[1])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.array(train_data)\n",
    "train_data=train_data/255.\n",
    "target=np.array(target)\n",
    "uni=np.unique(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = target.reshape(len(target), 1)\n",
    "Y = onehot_encoder.fit_transform(integer_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7301, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "test_data=[]\n",
    "\n",
    "with open('test_WyRytb0.csv') as dat:\n",
    "    dat.readline()\n",
    "    reader = csv.reader(dat, delimiter = ',')\n",
    "    for row in reader:\n",
    "        temp=cv2.imread(os.path.join(os.getcwd(), 'train', row[0]))\n",
    "        temp1=cv2.resize(temp, (100,100))\n",
    "        test_data.append(temp1)\n",
    "        \n",
    "        \n",
    "test_data=np.array(test_data)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IntelNet(input_shape = (100, 100, 3), classes = len(uni)):\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(128, (7, 7), strides = (2, 2), name = 'conv1',padding = 'SAME')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    #X = MaxPooling2D((3, 3), strides=(2, 2),padding = 'SAME')(X)\n",
    "    \n",
    "\n",
    "    X = Conv2D(128, (5, 5), strides = (1, 1), name = 'conv2')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv2')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    #X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    X = Conv2D(128, (5, 5), strides = (1, 1), name = 'conv3')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv3')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(256, (3, 3), strides = (1, 1), name = 'conv4')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv4')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IntelNet(input_shape = (100, 100, 3), classes = len(uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(train_data,Y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17034/17034 [==============================] - 139s 8ms/step - loss: 13.1769 - acc: 0.1776\n",
      "Epoch 2/100\n",
      "17034/17034 [==============================] - 118s 7ms/step - loss: 13.2444 - acc: 0.1783\n",
      "Epoch 3/100\n",
      "17034/17034 [==============================] - 113s 7ms/step - loss: 13.2444 - acc: 0.1783\n",
      "Epoch 4/100\n",
      "17034/17034 [==============================] - 112s 7ms/step - loss: 13.2444 - acc: 0.1783\n",
      "Epoch 5/100\n",
      "17034/17034 [==============================] - 113s 7ms/step - loss: 13.2444 - acc: 0.1783\n",
      "Epoch 6/100\n",
      "17034/17034 [==============================] - 111s 7ms/step - loss: 13.2444 - acc: 0.1783\n",
      "Epoch 7/100\n",
      "17034/17034 [==============================] - 112s 7ms/step - loss: 13.2444 - acc: 0.1783\n",
      "Epoch 8/100\n",
      "17034/17034 [==============================] - 114s 7ms/step - loss: 13.2444 - acc: 0.1783\n",
      "Epoch 9/100\n",
      "17034/17034 [==============================] - 113s 7ms/step - loss: 13.2444 - acc: 0.1783\n",
      "Epoch 10/100\n",
      "16000/17034 [===========================>..] - ETA: 6s - loss: 13.2440 - acc: 0.1783"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, Y, epochs = 100, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=model.predict(test_data)\n",
    "classe=probs.argmax(axis=-1)\n",
    "print(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_data=[]\n",
    "i=0\n",
    "with open('test_WyRytb0.csv') as dat:\n",
    "    dat.readline()\n",
    "    reader = csv.reader(dat, delimiter = ',')\n",
    "    for row in reader:\n",
    "        final_data.append([row[0], classe[i]])\n",
    "        i=i+1\n",
    "final_dataframe=pd.DataFrame(final_data,columns = ['image_name','label'])\n",
    "final_dataframe.to_csv('dl_result.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
